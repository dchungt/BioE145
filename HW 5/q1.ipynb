{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f42948a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def linear_forward(X, w, b):\n",
    "    \"\"\"\n",
    "    Q:  calculate the output of a node with no activation function: Z = Xw + b \n",
    "\n",
    "    Inputs\n",
    "    - X: input matrix (N, D)\n",
    "    - w: linear node weight matrix (D, D')\n",
    "    - b: the bias vector of shape (D', )\n",
    "    \"\"\"\n",
    "\n",
    "    Z = np.dot(X, w) + b\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c1c170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(X, w, b):\n",
    "    \"\"\"\n",
    "    Q:  with Z = Xw + b, find dZ/dw and dZ/db\n",
    "\n",
    "    Outputs a tuple of...\n",
    "    - (dZdw, dZdb)\n",
    "    \"\"\"\n",
    "    D, d = np.shape(w)\n",
    "    dZdw = X\n",
    "    dZdb = np.ones((d,1))\n",
    "    return dZdw, dZdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28110cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_forward(X):\n",
    "    \"\"\"\n",
    "    Q:  Z = relu(X) = max(X, 0)\n",
    "\n",
    "    Inputs\n",
    "    - X: input matrix (N, D)\n",
    "    \"\"\"\n",
    "    N, D = np.shape(X)\n",
    "    Z = np.zeros((N,D))\n",
    "    \n",
    "    for row in range(N):\n",
    "        for col in range(D):\n",
    "            Z[row][col] - max(X[row][col],0)\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "071ac50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(X):\n",
    "    \"\"\"\n",
    "    Q:  Z = relu(X) = max(X, 0), find dZ/dX\n",
    "\n",
    "    Inputs\n",
    "    - X: input matrix (N, D)\n",
    "    \"\"\"\n",
    "    N, D = np.shape(X)\n",
    "    Z = np.zeros((N,D))\n",
    "    \n",
    "    for row in range(N):\n",
    "        for col in range(D):\n",
    "            if X[row][col] > 0:\n",
    "                Z[row][col] = 1\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ec51c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_forward(X):\n",
    "    \"\"\"\n",
    "    Q:  Z = softmax(X)\n",
    "\n",
    "    Inputs\n",
    "    - X: input matrix (N, D)\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    Z = np.zeros((N, D))\n",
    "    \n",
    "    for d in range (N):\n",
    "        bottom = np.sum([np.exp(item) for item in X[d]])\n",
    "        for feature in range(D):\n",
    "            top = np.exp(X[d][feature])\n",
    "            Z[d][feature] = top / bottom\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9719520e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(X):\n",
    "    \"\"\"\n",
    "    Q:  Z = softmax(X), find dZ/dX\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    softmax = softmax_forward(X)\n",
    "    softmax_b = np.empty((N,D,D))\n",
    "    for i in range(N):\n",
    "        for j in range (D):\n",
    "            for k in range(D):\n",
    "                if j==k:\n",
    "                    softmax_b[i,j,k] = softmax[i,j] * (1 - softmax[i,k])\n",
    "                else:\n",
    "                    softmax_b[i,j,k] = -softmax[i,j] * softmax[i,k]\n",
    "    return softmax_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "650dc7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_test\n",
      "[[ 0.17053049 -0.49028621  1.24210294  0.16607946  0.5155312   0.96254506]\n",
      " [-0.66000863 -0.08221805 -0.23124341  0.55491303 -0.21842568  0.3191322 ]\n",
      " [-0.36904111  0.60467294  0.62060448  1.66314573 -0.10032968  0.58519508]\n",
      " [-0.34748938 -1.14078261  0.09789411 -0.6842616   0.43958586 -0.0521021 ]\n",
      " [-0.16206085 -0.64226044  0.92800334  0.48579765 -0.31495605 -0.2972579 ]]\n",
      "relu_test\n",
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "softmax_test\n",
      "[[0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]]\n",
      "array([[[ 0.13888889, -0.02777778, -0.02777778, -0.02777778,\n",
      "         -0.02777778, -0.02777778],\n",
      "        [-0.02777778,  0.13888889, -0.02777778, -0.02777778,\n",
      "         -0.02777778, -0.02777778],\n",
      "        [-0.02777778, -0.02777778,  0.13888889, -0.02777778,\n",
      "         -0.02777778, -0.02777778],\n",
      "        [-0.02777778, -0.02777778, -0.02777778,  0.13888889,\n",
      "         -0.02777778, -0.02777778],\n",
      "        [-0.02777778, -0.02777778, -0.02777778, -0.02777778,\n",
      "          0.13888889, -0.02777778],\n",
      "        [-0.02777778, -0.02777778, -0.02777778, -0.02777778,\n",
      "         -0.02777778,  0.13888889]],\n",
      "\n",
      "       [[ 0.13888889, -0.02777778, -0.02777778, -0.02777778,\n",
      "         -0.02777778, -0.02777778],\n",
      "        [-0.02777778,  0.13888889, -0.02777778, -0.02777778,\n",
      "         -0.02777778, -0.02777778],\n",
      "        [-0.02777778, -0.02777778,  0.13888889, -0.02777778,\n",
      "         -0.02777778, -0.02777778],\n",
      "        [-0.02777778, -0.02777778, -0.02777778,  0.13888889,\n",
      "         -0.02777778, -0.02777778],\n",
      "        [-0.02777778, -0.02777778, -0.02777778, -0.02777778,\n",
      "          0.13888889, -0.02777778],\n",
      "        [-0.02777778, -0.02777778, -0.02777778, -0.02777778,\n",
      "         -0.02777778,  0.13888889]],\n",
      "\n",
      "       [[ 0.13888889, -0.02777778, -0.02777778, -0.02777778,\n",
      "         -0.02777778, -0.02777778],\n",
      "        [-0.02777778,  0.13888889, -0.02777778, -0.02777778,\n",
      "         -0.02777778, -0.02777778],\n",
      "        [-0.02777778, -0.02777778,  0.13888889, -0.02777778,\n",
      "         -0.02777778, -0.02777778],\n",
      "        [-0.02777778, -0.02777778, -0.02777778,  0.13888889,\n",
      "         -0.02777778, -0.02777778],\n",
      "        [-0.02777778, -0.02777778, -0.02777778, -0.02777778,\n",
      "          0.13888889, -0.02777778],\n",
      "        [-0.02777778, -0.02777778, -0.02777778, -0.02777778,\n",
      "         -0.02777778,  0.13888889]],\n",
      "\n",
      "       [[ 0.13888889, -0.02777778, -0.02777778, -0.02777778,\n",
      "         -0.02777778, -0.02777778],\n",
      "        [-0.02777778,  0.13888889, -0.02777778, -0.02777778,\n",
      "         -0.02777778, -0.02777778],\n",
      "        [-0.02777778, -0.02777778,  0.13888889, -0.02777778,\n",
      "         -0.02777778, -0.02777778],\n",
      "        [-0.02777778, -0.02777778, -0.02777778,  0.13888889,\n",
      "         -0.02777778, -0.02777778],\n",
      "        [-0.02777778, -0.02777778, -0.02777778, -0.02777778,\n",
      "          0.13888889, -0.02777778],\n",
      "        [-0.02777778, -0.02777778, -0.02777778, -0.02777778,\n",
      "         -0.02777778,  0.13888889]],\n",
      "\n",
      "       [[ 0.13888889, -0.02777778, -0.02777778, -0.02777778,\n",
      "         -0.02777778, -0.02777778],\n",
      "        [-0.02777778,  0.13888889, -0.02777778, -0.02777778,\n",
      "         -0.02777778, -0.02777778],\n",
      "        [-0.02777778, -0.02777778,  0.13888889, -0.02777778,\n",
      "         -0.02777778, -0.02777778],\n",
      "        [-0.02777778, -0.02777778, -0.02777778,  0.13888889,\n",
      "         -0.02777778, -0.02777778],\n",
      "        [-0.02777778, -0.02777778, -0.02777778, -0.02777778,\n",
      "          0.13888889, -0.02777778],\n",
      "        [-0.02777778, -0.02777778, -0.02777778, -0.02777778,\n",
      "         -0.02777778,  0.13888889]]])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Softmax gradient incorrect!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sv/38pg6r6n7cv1p540dxmhhmtw0000gn/T/ipykernel_19096/2348284446.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    107\u001b[0m                     [-0.01503696, -0.01503696, -0.0380353 , -0.02444215, -0.01503696,  0.10758833]]])\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msm\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msoftmax_back_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Softmax gradient incorrect!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mrelu_back_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelu_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Softmax gradient incorrect!"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # due to popular demand, we have increased the number and clarity of test cases available to you\n",
    "\n",
    "    # we will use this (5, 8) matrix to test all the functions above!\n",
    "    X = np.array([[-0.38314889,  0.35891731,  0.09037955,  0.98397352, -0.74292248, -0.5883056,  0.54354937,  0.79001348],\n",
    "                  [ 0.58758113, -0.412598  ,  0.08740239, -0.68723605, -0.29251551, 0.36521658, -0.25330565,  0.03919754],\n",
    "                  [ 0.97960327, -0.41368028,  0.26308195,  0.94303171, -0.92383705, 0.28187289,  0.35914219, -0.46526478],\n",
    "                  [ 0.2583081 ,  0.97956892,  0.31049517, -0.68557195, -0.68612885, -0.9054485, -0.70507179,  0.11431403],\n",
    "                  [-0.7674351 ,  0.69421738, -0.8007104 ,  0.93470719,  0.61132148, 0.54328029,  0.00919623, -0.34544161]])\n",
    "\n",
    "    # predefined weights and biases to ease your testing\n",
    "    w = np.array([[0.77805922, 0.67805674, 0.18799035, 0.93644034, 0.87466635, 0.66450703],\n",
    "                  [0.86038224, 0.21901606, 0.87774923, 0.21039304, 0.76061141, 0.37033866],\n",
    "                  [0.49032109, 0.71247207, 0.61826719, 0.37348737, 0.4197679 , 0.70488014],\n",
    "                  [0.37720786, 0.39471295, 0.68555261, 0.48458372, 0.29309447, 0.01436672],\n",
    "                  [0.68969515, 0.10709357, 0.02608303, 0.35893371, 0.53729841, 0.53873035],\n",
    "                  [0.31109099, 0.99274133, 0.78935902, 0.77859174, 0.02639908, 0.17466261],\n",
    "                  [0.30502676, 0.07085277, 0.03068556, 0.4183926 , 0.07385148, 0.99708494],\n",
    "                  [0.87156768, 0.47651573, 0.76058837, 0.1566234 , 0.95023629, 0.78754312]])\n",
    "    b = np.array([-0.41458132, -0.5132465, 0.13485534, 0.31234684, -0.1248132, 0.34524132])\n",
    "\n",
    "    N, D = X.shape\n",
    "\n",
    "    # forward pass\n",
    "    linear_test = linear_forward(X, w, b)\n",
    "    print(\"linear_test\")\n",
    "    print(linear_test)\n",
    "\n",
    "    \"\"\"\n",
    "    linear_test:\n",
    "    array([[ 0.17053049 -0.49028621  1.24210294  0.16607946  0.5155312   0.96254506]\n",
    "           [-0.66000863 -0.08221805 -0.23124341  0.55491303 -0.21842568  0.3191322 ]\n",
    "           [-0.36904111  0.60467294  0.62060448  1.66314573 -0.10032968  0.58519508]\n",
    "           [-0.34748938 -1.14078261  0.09789411 -0.6842616   0.43958586 -0.0521021 ]\n",
    "           [-0.16206085 -0.64226044  0.92800334  0.48579765 -0.31495605 -0.2972579 ]])\n",
    "    \"\"\"\n",
    "\n",
    "    relu_test = relu_forward(linear_test)\n",
    "    print(\"relu_test\")\n",
    "    print(relu_test)\n",
    "\n",
    "    \"\"\"\n",
    "    relu_test:\n",
    "    array([[0.17053049 0.         1.24210294 0.16607946 0.5155312  0.96254506]\n",
    "          [0.         0.         0.         0.55491303 0.         0.3191322 ]\n",
    "          [0.         0.60467294 0.62060448 1.66314573 0.         0.58519508]\n",
    "          [0.         0.         0.09789411 0.         0.43958586 0.        ]\n",
    "          [0.         0.         0.92800334 0.48579765 0.         0.        ]])\n",
    "    \"\"\"\n",
    "\n",
    "    softmax_test = softmax_forward(relu_test)\n",
    "    print(\"softmax_test\")\n",
    "    print(softmax_test)\n",
    "\n",
    "    \"\"\"\n",
    "    softmax:\n",
    "    array([[0.10662601 0.08990891 0.31134448 0.10615247 0.15055496 0.23541316]\n",
    "           [0.14049437 0.14049437 0.14049437 0.24471163 0.14049437 0.19331088]\n",
    "           [0.07835807 0.14344646 0.14575008 0.41340786 0.07835807 0.14067947]\n",
    "           [0.15026499 0.15026499 0.16571914 0.15026499 0.23322092 0.15026499]\n",
    "           [0.12262529 0.12262529 0.31017499 0.19932386 0.12262529 0.12262529]])\n",
    "    \"\"\"\n",
    "\n",
    "    assert np.all(np.abs(np.sum(softmax_test, axis=1) - 1.0) < 1e-10), \"Rows of softmax output need to sum to 1!\"\n",
    "\n",
    "    # backward pass\n",
    "    softmax_back_test = softmax_backward(relu_test)\n",
    "    print(softmax_back_test.__repr__())\n",
    "\n",
    "    \"\"\"\n",
    "    softmax_back_test:\n",
    "    \"\"\"\n",
    "    sm = np.array([[[ 0.09525691, -0.00958663, -0.03319742, -0.01131862, -0.01605308, -0.02510117],\n",
    "                    [-0.00958663,  0.0818253 , -0.02799264, -0.00954405, -0.01353623, -0.02116574],\n",
    "                    [-0.03319742, -0.02799264,  0.2144091 , -0.03304999, -0.04687446, -0.07329459],\n",
    "                    [-0.01131862, -0.00954405, -0.03304999,  0.09488413, -0.01598178, -0.02498969],\n",
    "                    [-0.01605308, -0.01353623, -0.04687446, -0.01598178, 0.12788817, -0.03544262],\n",
    "                    [-0.02510117, -0.02116574, -0.07329459, -0.02498969, -0.03544262,  0.1799938 ]],\n",
    "\n",
    "                   [[ 0.12075571, -0.01973867, -0.01973867, -0.03438061, -0.01973867, -0.02715909],\n",
    "                    [-0.01973867,  0.12075571, -0.01973867, -0.03438061, -0.01973867, -0.02715909],\n",
    "                    [-0.01973867, -0.01973867,  0.12075571, -0.03438061, -0.01973867, -0.02715909],\n",
    "                    [-0.03438061, -0.03438061, -0.03438061,  0.18482785, -0.03438061, -0.04730542],\n",
    "                    [-0.01973867, -0.01973867, -0.01973867, -0.03438061, 0.12075571, -0.02715909],\n",
    "                    [-0.02715909, -0.02715909, -0.02715909, -0.04730542, -0.02715909,  0.15594178]],\n",
    "\n",
    "                   [[ 0.07221808, -0.01124019, -0.01142069, -0.03239384, -0.00613999, -0.01102337],\n",
    "                    [-0.01124019,  0.12286957, -0.02090733, -0.05930189, -0.01124019, -0.02017997],\n",
    "                    [-0.01142069, -0.02090733,  0.124507  , -0.06025423, -0.01142069, -0.02050404],\n",
    "                    [-0.03239384, -0.05930189, -0.06025423,  0.2425018 , -0.03239384, -0.058158  ],\n",
    "                    [-0.00613999, -0.01124019, -0.01142069, -0.03239384, 0.07221808, -0.01102337],\n",
    "                    [-0.01102337, -0.02017997, -0.02050404, -0.058158  , -0.01102337,  0.12088875]],\n",
    "\n",
    "                   [[ 0.12768542, -0.02257957, -0.02490178, -0.02257957, -0.03504494, -0.02257957],\n",
    "                    [-0.02257957,  0.12768542, -0.02490178, -0.02257957, -0.03504494, -0.02257957],\n",
    "                    [-0.02490178, -0.02490178,  0.13825631, -0.02490178, -0.03864917, -0.02490178],\n",
    "                    [-0.02257957, -0.02257957, -0.02490178,  0.12768542, -0.03504494, -0.02257957],\n",
    "                    [-0.03504494, -0.03504494, -0.03864917, -0.03504494, 0.17882892, -0.03504494],\n",
    "                    [-0.02257957, -0.02257957, -0.02490178, -0.02257957, -0.03504494,  0.12768542]],\n",
    "\n",
    "                   [[ 0.10758833, -0.01503696, -0.0380353 , -0.02444215, -0.01503696, -0.01503696],\n",
    "                    [-0.01503696,  0.10758833, -0.0380353 , -0.02444215, -0.01503696, -0.01503696],\n",
    "                    [-0.0380353 , -0.0380353 ,  0.21396646, -0.06182527, -0.0380353 , -0.0380353 ],\n",
    "                    [-0.02444215, -0.02444215, -0.06182527,  0.15959386, -0.02444215, -0.02444215],\n",
    "                    [-0.01503696, -0.01503696, -0.0380353 , -0.02444215, 0.10758833, -0.01503696],\n",
    "                    [-0.01503696, -0.01503696, -0.0380353 , -0.02444215, -0.01503696,  0.10758833]]])\n",
    "\n",
    "    assert np.all(np.abs(sm - softmax_back_test) < 1e-8), \"Softmax gradient incorrect!\"\n",
    "\n",
    "    relu_back_test = relu_backward(relu_test)\n",
    "    print(\"relu_back_test\")\n",
    "    print(relu_back_test.__repr__())\n",
    "    \"\"\"\n",
    "    relu_back_test:\n",
    "    \"\"\"\n",
    "    rbt = np.array([[1., 0., 1., 1., 1., 1.],\n",
    "                    [0., 0., 0., 1., 0., 1.],\n",
    "                    [0., 1., 1., 1., 0., 1.],\n",
    "                    [0., 0., 1., 0., 1., 0.],\n",
    "                    [0., 0., 1., 1., 0., 0.]])\n",
    "    assert np.all(np.abs(rbt - relu_back_test) < 1e-20), \"relu gradient incorrect!\"\n",
    "\n",
    "    linear_back_test, bias_back_test = linear_backward(X, w, b)\n",
    "    print(\"linear_back_test\")\n",
    "    print(linear_back_test)\n",
    "    lbt = np.array([[-0.38314889,  0.35891731,  0.09037955,  0.98397352, -0.74292248, -0.5883056,  0.54354937,  0.79001348],\n",
    "                    [ 0.58758113, -0.412598  ,  0.08740239, -0.68723605, -0.29251551, 0.36521658, -0.25330565,  0.03919754],\n",
    "                    [ 0.97960327, -0.41368028,  0.26308195,  0.94303171, -0.92383705, 0.28187289,  0.35914219, -0.46526478],\n",
    "                    [ 0.2583081 ,  0.97956892,  0.31049517, -0.68557195, -0.68612885, -0.9054485, -0.70507179,  0.11431403],\n",
    "                    [-0.7674351 ,  0.69421738, -0.8007104 ,  0.93470719,  0.61132148, 0.54328029,  0.00919623, -0.34544161]])\n",
    "    assert np.all(np.abs(lbt - linear_back_test) < 1e-10), \"Linear gradient incorrect!\"\n",
    "    print(\"bias\")\n",
    "    print(bias_back_test)   # should be 1\n",
    "\n",
    "    print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a034786a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
