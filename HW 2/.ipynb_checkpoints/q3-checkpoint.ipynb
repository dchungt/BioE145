{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "School: University of California, Berkeley<br>\n",
    "Course: BIOENG 145/245<br>\n",
    "Author: Yorick Chern<br>\n",
    "Instructor: Liana Lareau<br>\n",
    "Assignment 2<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import multivariate_normal\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some helper functions - do not change anything here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf(x, mean, cov):\n",
    "    return multivariate_normal.pdf(x, mean, cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(n_samples, features, k_classes, sep=2.0):\n",
    "    centers = np.random.rand(k_classes, features)\n",
    "    for i in range(k_classes):\n",
    "        centers[i] += sep * i\n",
    "    stds = np.ones_like(centers)\n",
    "    X, y = make_blobs(\n",
    "        n_samples=n_samples,\n",
    "        n_features=features,\n",
    "        centers=centers,\n",
    "        cluster_std=stds\n",
    "    )\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDiscriminator:\n",
    "    def __init__(self):\n",
    "        self.gaussians = []     # stores the (class_label, mean, covariance) tuples\n",
    "    def fit(self, X, y, num_classes):\n",
    "        \"\"\"\n",
    "        This function fits a Gaussian distribution to each class in the dataset.\n",
    "\tThese Gaussian distributions are appended to self.gaussians as a tuple consisting of (class_label, mean, covariance)\n",
    "        Inputs\n",
    "        - X: the data, numpy array with shape (n, d) = (sample size, feature dims)\n",
    "        - y: the class labels, numpy array with shape (n, ) and integers from 0, ..., num_classes-1 \n",
    "        - num_classes: the number of classes, int\n",
    "        Outputs\n",
    "        - None\n",
    "        \"\"\"\n",
    "\n",
    "        # iterate through each class and find its mean and covariance matrix\n",
    "        # HINT: use np.mean, np.cov, and np.where.\n",
    "        # save the class k, the mean of the class sample, and covariance matrix of the class sample as a tuple\n",
    "        # and append it to self.gaussians\n",
    "        # self.gaussians.append((k, mean, cov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        This function will classify the dataset x, returning a predicted label for each datapoint.\n",
    "        Inputs\n",
    "        - x: data to be predicted, numpy array with shape (n, d) = (sample size, feature dims)\n",
    "        Outputs\n",
    "        - pred: np.array with shape (n, ), where each item is the classification of each data\n",
    "        \"\"\"\n",
    "        pred = []\n",
    "        # iterate through each data\n",
    "        for i in range(x.shape[0]):\n",
    "        \t# for each data iterate through each gaussian in self.gaussian\n",
    "        \t# find the pdf using the helper function provided above\n",
    "        \t# HINT: remember, in Python, tuples can be decomposed like so\n",
    "        \t# a, b, c = (a, b, c)\n",
    "        \t...\n",
    "        return np.array(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t# do not make any modifications here, it's a simple visualization for your implementation.\n",
    "    X, y = make_data(1200, 2, 2, sep=3.0)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    idx = np.where(y_train == 0)[0]\n",
    "    plt.scatter(X_train[idx, 0], X_train[idx, 1], label='0', color='r')\n",
    "    idx = np.where(y_train == 1)[0]\n",
    "    plt.scatter(X_train[idx, 0], X_train[idx, 1], label='1', color='b')\n",
    "    plt.legend()\n",
    "    plt.title(\"Train data\")\n",
    "    plt.show()  # click on \"x\" to exit out the graph and continue running the program\n",
    "    gd = GaussianDiscriminator()\n",
    "    gd.fit(X_train, y_train, 2)\n",
    "    y_pred = gd.predict(X_test)\n",
    "    print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    idx = np.where(y_test == 0)[0]\n",
    "    plt.scatter(X_test[idx, 0], X_test[idx, 1], label='0', color='r')\n",
    "    idx = np.where(y_test == 1)[0]\n",
    "    plt.scatter(X_test[idx, 0], X_test[idx, 1], label='1', color='b')\n",
    "    plt.legend()\n",
    "    plt.title(\"Test data & predictions\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
