{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b656ad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "School: University of California, Berkeley\n",
    "Course: BIOENG 145/245\n",
    "Author: Yorick Chern\n",
    "Instructor: Liana Lareau\n",
    "Assignment 4\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dddce64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph(X, y, title=\"\"):\n",
    "    plt.scatter(X.flatten(), y.flatten(), marker=\".\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def graph_line(X, y, w, b):\n",
    "    plt.plot(X.flatten(), X @ w + b, color=\"red\")\n",
    "    plt.scatter(X.flatten(), y.flatten(), marker=\".\")\n",
    "    plt.title(f\"Linear Regression Results: y = {np.round(w.flatten()[0], 2)}x + {np.round(b.flatten()[0], 2)}\")\n",
    "    plt.show()\n",
    "\n",
    "def graph_losses(losses):\n",
    "    epochs = [i for i in range(len(losses))]\n",
    "    plt.plot(epochs, losses)\n",
    "    plt.title(\"Losses vs. Epochs\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7489b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "\n",
    "    def __init__(self, gd='mb'):\n",
    "        \"\"\"\n",
    "        The initiatialization function. (Implemented for you).\n",
    "        \n",
    "        Inputs\n",
    "        - gd: can be 'mb' for mini-batch, 'b' for batch, or 'sgd' for stochastic gradient descent\n",
    "        \"\"\"\n",
    "        self.gd = gd\n",
    "        assert self.gd in [\"mb\", \"b\", \"sgd\"], \"gd argument needs to be one of \\\"mb\\\", \\\"b\\\", or \\\"sgd\\\"!\"\n",
    "\n",
    "    def fit(self, X, y, epochs=10, lr=1e-3, batch_size=32):\n",
    "        \"\"\"\n",
    "        Q: this is the main function that will run the entire gradient descent algorithm.\n",
    "\n",
    "        Inputs\n",
    "        - X: data matrix (N, D)\n",
    "        - y: ground truth (N, )\n",
    "        - epochs: the number of epochs to train for\n",
    "        - lr: the learning rate\n",
    "        - batch_size: the size of the batch to use\n",
    "\n",
    "        Outputs:\n",
    "        - losses: list of losses over epochs\n",
    "        \"\"\"\n",
    "        N, D = X.shape\n",
    "        y = y.reshape(N, 1)     # we will reshape y to avoid broadcasting errors\n",
    "        self.theta = np.zeros((D, 1))    # initialize weights to zero\n",
    "        self.bias = np.zeros((1, 1))     # initialize bias to zero\n",
    "\n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "\n",
    "            \n",
    "            if self.gd == \"b\":\n",
    "                dldw, dldb = self.mean_squared_gradient(X, y)\n",
    "\n",
    "                # update parameters\n",
    "                self.theta -= lr * dldw\n",
    "                self.bias -= lr * dldb\n",
    "\n",
    "                epoch_loss += self.mean_squared_error(X, y)\n",
    "\n",
    "            elif self.gd == \"mb\":\n",
    "                num_batches = int(np.ceil(N / batch_size))\n",
    "                for batch in range(num_batches):\n",
    "                    start = batch * batch_size\n",
    "                    end = min(start + batch_size, N)\n",
    "                    X_batch = X[start:end]\n",
    "                    y_batch = y[start:end]\n",
    "\n",
    "                    dldw, dldb = self.mean_squared_gradient(X_batch, y_batch)\n",
    "\n",
    "                    # update parameters\n",
    "                    self.theta -= lr * dldw\n",
    "                    self.bias -= lr * dldb\n",
    "\n",
    "                    epoch_loss += self.mean_squared_error(X_batch, y_batch) * (end - start)\n",
    "\n",
    "            elif self.gd == \"sgd\":\n",
    "                indices = np.random.permutation(N)\n",
    "                for i in indices:\n",
    "                    X_i = X[i:i+1]\n",
    "                    y_i = y[i:i+1]\n",
    "    \n",
    "                    dldw, dldb = self.mean_squared_gradient(X_i, y_i)\n",
    "\n",
    "                    # update parameters\n",
    "                    self.theta -= lr * dldw\n",
    "                    self.bias -= lr * dldb\n",
    "\n",
    "                    epoch_loss += self.mean_squared_error(X_i, y_i)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Invalid gradient descent algorithm.\")\n",
    "\n",
    "            losses.append(epoch_loss / N)\n",
    "\n",
    "        return self.theta, self.bias, losses\n",
    "\n",
    "    def mean_squared_error(self, X, y):\n",
    "        \"\"\"\n",
    "        Q:  use X, y, and self.theta to calculate the mean-squared-error.\n",
    "\n",
    "        Inputs\n",
    "        - X: data matrix (N, D)\n",
    "        - y: ground truth (N, 1)\n",
    "\n",
    "        Outputs:\n",
    "        - loss: loss\n",
    "        \"\"\"\n",
    "\n",
    "        y_hat = np.dot(X, self.theta) + self.bias\n",
    "        total_loss = np.sum(np.square(y_hat - y))\n",
    "        loss = total_loss/len(X)\n",
    "        return loss\n",
    "\n",
    "    def mean_squared_gradient(self, X_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Q:  calculate the gradient of the mean-squared-error w.r.t. self.theta, the weights,\n",
    "        and self.bias, the bias\n",
    "\n",
    "        Inputs\n",
    "        - X_batch: data matrix (batch_size, D)\n",
    "        - y_batch: ground truth (batch_size, 1)\n",
    "\n",
    "        Outputs:\n",
    "        - dldw: gradient w.r.t. weights should be of shape (D, 1)\n",
    "        - dldb: gradient w.r.t. bias should be of shape (1, 1)\n",
    "        \"\"\"\n",
    "        batch_size, D = X_batch.shape\n",
    "        \n",
    "        y_hat = np.dot(X_batch, self.theta) + self.bias\n",
    "        dldw = np.dot(np.transpose(X_batch), (2 * (y_hat - y_batch))) / batch_size\n",
    "        dldb = (np.sum(2 * (y_hat - y_batch)) / batch_size)\n",
    "        assert dldw.shape == (D, 1), f\"dldw shape not {D, 1}, but it is {dldw.shape}\"\n",
    "        return dldw, dldb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00d2ce1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9715251767172561\n",
      "(array([[1.4944319 ],\n",
      "       [1.05027943],\n",
      "       [1.42694079],\n",
      "       [1.20808437],\n",
      "       [1.69532283]]), 2.5683096567102917)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # this section tests each function (besides lin_reg.fit()) individually\n",
    "    X = np.array([[0.48553452, 0.02705233, 0.59605814, 0.50225229, 0.81243239],\n",
    "       [0.17469252, 0.25435186, 0.19408888, 0.17743825, 0.13085249],\n",
    "       [0.28457151, 0.4112227 , 0.37798194, 0.38538792, 0.52813069],\n",
    "       [0.88580912, 0.67981496, 0.68811359, 0.01019072, 0.24051601],\n",
    "       [0.43847827, 0.40985491, 0.44461599, 0.38423286, 0.98673558],\n",
    "       [0.85719302, 0.11396699, 0.88914111, 0.95473146, 0.98221489],\n",
    "       [0.41547788, 0.22504066, 0.96959725, 0.34318878, 0.76847212],\n",
    "       [0.749719  , 0.70648943, 0.1026854 , 0.62488796, 0.67939783],\n",
    "       [0.16971717, 0.89160743, 0.44127767, 0.33990768, 0.28538788],\n",
    "       [0.42643292, 0.50454711, 0.30491342, 0.19072025, 0.3468241 ]])\n",
    "\n",
    "    y = np.array([[0.52060233],\n",
    "       [0.48235778],\n",
    "       [0.47203745],\n",
    "       [0.51458632],\n",
    "       [0.70178445],\n",
    "       [0.17644808],\n",
    "       [0.82841289],\n",
    "       [0.7181598 ],\n",
    "       [0.80074196],\n",
    "       [0.50740697]])\n",
    "\n",
    "    lin_reg = LinearRegression()\n",
    "    lin_reg.theta = np.array([[0.96348654],\n",
    "       [0.97798831],\n",
    "       [0.47966334],\n",
    "       [0.65893575],\n",
    "       [0.72210114]])\n",
    "    lin_reg.bias = np.array([[0.05831978]])\n",
    "\n",
    "    print(lin_reg.mean_squared_error(X, y))  # solution result: 1.971525196568432\n",
    "    print(lin_reg.mean_squared_gradient(X, y))\n",
    "\n",
    "    \"\"\"\n",
    "    mean_squared_gradient should look like this\n",
    "    (array([[1.4944319 ],\n",
    "       [1.05027943],\n",
    "       [1.42694079],\n",
    "       [1.20808437],\n",
    "       [1.69532283]]), 2.5683096567102917)\n",
    "    \"\"\"\n",
    "\n",
    "    # this section tests the entire class, aka the fit function and produces visualization results\n",
    "    # N = 200\n",
    "    # D = 1\n",
    "    # X = np.random.rand(N, D) * 20\n",
    "\n",
    "    # # you can change these 3 numbers - this is the line we want to regress\n",
    "    # # y = wx + b + noise\n",
    "    # theta_set = -0.42\n",
    "    # b_set = -0.13\n",
    "    # noise = np.random.randn(N, D) * 0.50\n",
    "\n",
    "    # graph_progress = True      # turn this to True to visualize your code's progress\n",
    "\n",
    "    # y = X * theta_set + b_set + noise   # change the coefficients here\n",
    "\n",
    "\n",
    "    # lin_reg = LinearRegression(gd='mb')\n",
    "    # theta, bias, losses = lin_reg.fit(X, y, lr=5e-5, epochs=1000)   # these parameters work the best\n",
    "\n",
    "    # if graph_progress:\n",
    "    #     graph(X, y, title=f\"y = {theta_set}x + {b_set} + noise\")\n",
    "    #     graph_line(X, y, theta, bias)\n",
    "    #     graph_losses(losses)\n",
    "\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163b4e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
